<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<style>
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>
<title>Druid Cluster Performance Report</title>
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['$$$','$$$']]}});</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<h1>Druid Cluster Performance Report</h1>

<h2>Background</h2>

<p>Druid has five key components: <strong>Historical Node</strong>, <strong>Realtime Node</strong>, <strong>Coordinator Node</strong>, <strong>Broker Node</strong>, and <strong>Indexer Node</strong>. In this report, we didn't setup Realtime nodes and  builded index only through batch way.</p>

<p>Aside from these nodes, there are 3 external dependencies to the system : <strong>ZooKeeper</strong>, <strong>MySQL instance</strong>, <strong>"deep storage"</strong>. We used HDFS as "deep storage".</p>

<p>Get more detailed description of these components, please visit the official website: <a href="http://druid.io/docs/0.6.105/Design.html">Design of Druid</a> or the <a href="http://static.druid.io/docs/druid.pdf">White paper</a>.</p>

<p>This performance report is based on the application of <a href="https://docs.google.com/a/yahoo-inc.com/document/d/1O46sJhNWI-B5ToXY96BGPTNZZNCAXH633PHKz2b3GaM/edit?usp=drive_web">D&amp;A Analytic Tool</a>.</p>

<h3>Data Set</h3>

<p><strong>89G</strong> data containing 59837541 instances with Json format. We imported the data into HDFS, and build index through a Hadoop job.</p>

<p>Size of the data is too large to build index in local model. We tried to setup a standalone Hadoop cluster to build index, it failed due to out of memory of JVM. We also tried to optimize the setting of JVM memory and count of MR tasks, it still could't work. So a distributed Hadoop cluster is needed.</p>

<h2>Cluster Setup</h2>

<h4>Machine Summary</h4>

<pre>
Summary:    HP DL160 G6, 2 x Xeon X5650 2.67GHz, 15.7GB / 16GB 1333MHz DDR3, 1 x 500GB SATA
System:     HP ProLiant DL160 G6, C-2P/16/500, ySPEC 39.5
Processors: 2 x Xeon X5650 2.67GHz (HT enabled, 12 cores, 24 threads) - Gulftown B1, 64-bit, six-core, 32nm, L3: 12MB
Memory:     15.7GB / 16GB 1333MHz DDR3 == 4 x 4GB - 4GB PC3-10600 Hynix DDR3-1333 ECC Registered CL9 2Rx4
                                      14 x empty
Disk:       sda (ahci0): 500GB (33%) JBOD == 1 x 500GB 7.2K SATA/300 HP/Seagate Constellation ES 32MB
Disk-Control:   ahci0: HP/Intel ICH10 82801J 6 Port SATA/300 AHCI
Chipset:    Intel 5520 IOH-36D B3 (Tylersburg), 82801JIR A0 (ICH10R)
Network:    eth0 (igb): HP NC362i/Intel 82576 Gigabit, 1Gb/s <full-duplex>
Network:    eth1 (igb): HP NC362i/Intel 82576 Gigabit, no carrier
OS:     YLinux 6.5.0_84, RHEL Server 6.5, Linux 2.6.32-431.3.1.el6.YAHOO.20140110.x86_64 x86_64, 64-bit
BIOS:       HP O33 08/16/2010, rev 8.15
Hostname:   raptorsim0017.rm.bf1.yahoo.com
</pre>


<h4>Druid Service Cluster</h4>

<ul>
<li>10 Historical Nodes;</li>
<li>1 Zookeeper Node;</li>
<li>1 Broker Node;</li>
<li>1 Mysql;</li>
<li>1 Coodinator Node;</li>
<li>1 index Node</li>
<li>Deep Storage(HDFS)</li>
</ul>


<table>
<thead>
<tr>
<th align="center">No.</th>
<th align="center">Nodes Hostname </th>
<th align="center"> Role </th>
<th align="center"> Dependencies</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1 </td>
<td align="center">raptorsim0008.rm.bf1.yahoo.com </td>
<td align="center"> Coodinator, Historical </td>
<td align="center"> Hadoop Client, Mysql, Zookeeper</td>
</tr>
<tr>
<td align="center">2 </td>
<td align="center">raptorsim0009.rm.bf1.yahoo.com </td>
<td align="center"> Mysql, Historical     </td>
<td align="center"> Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">3 </td>
<td align="center">raptorsim0010.rm.bf1.yahoo.com </td>
<td align="center"> Zookeeper, Historical  </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">4 </td>
<td align="center">raptorsim0011.rm.bf1.yahoo.com </td>
<td align="center"> Broker     </td>
<td align="center">Mysql, Zookeeper</td>
</tr>
<tr>
<td align="center">5 </td>
<td align="center">raptorsim0012.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">6 </td>
<td align="center">raptorsim0013.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">7 </td>
<td align="center">raptorsim0014.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">8 </td>
<td align="center">raptorsim0015.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">9 </td>
<td align="center">raptorsim0016.rm.bf1.yahoo.com </td>
<td align="center"> index, Hadoop Master </td>
<td align="center">Hadoop, Mysql, Zookeeper</td>
</tr>
<tr>
<td align="center">10</td>
<td align="center">raptorsim0017.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">11</td>
<td align="center">raptorsim0018.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
<tr>
<td align="center">12</td>
<td align="center">raptorsim0019.rm.bf1.yahoo.com </td>
<td align="center"> Historical </td>
<td align="center">Hadoop Client, Zookeeper</td>
</tr>
</tbody>
</table>


<h4>Hadoop[yarn] Index Builder Cluster</h4>

<table>
<thead>
<tr>
<th align="center">No.</th>
<th align="center">Nodes Hostname </th>
<th align="center"> Role </th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1 </td>
<td align="center">raptorsim0012.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
<tr>
<td align="center">2 </td>
<td align="center">raptorsim0013.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
<tr>
<td align="center">3 </td>
<td align="center">raptorsim0014.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
<tr>
<td align="center">4 </td>
<td align="center">raptorsim0015.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
<tr>
<td align="center">5 </td>
<td align="center">raptorsim0016.rm.bf1.yahoo.com </td>
<td align="center"> Master, Druid Indexer</td>
</tr>
<tr>
<td align="center">6 </td>
<td align="center">raptorsim0017.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
<tr>
<td align="center">7 </td>
<td align="center">raptorsim0018.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
<tr>
<td align="center">8 </td>
<td align="center">raptorsim0019.rm.bf1.yahoo.com </td>
<td align="center"> Slave</td>
</tr>
</tbody>
</table>


<h4>Hadoop(yarn) Setting</h4>

<p>MIN_CONTAINER_SIZE should be 1024 MB if total RAM per Node is between 8 GB and 24 GB.</p>

<p>containers = min (2*CORES, 1.8*DISKS, (Total available RAM) / MIN_CONTAINER_SIZE) = 5</p>

<p>RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers)) = 2048m</p>

<table>
<thead>
<tr>
<th align="center">No.</th>
<th align="center">Configuration File </th>
<th align="center"> Configuration Setting </th>
<th align="center"> Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1 </td>
<td align="center">yarn-site.xml    </td>
<td align="center"> yarn.nodemanager.resource.memory-mb    </td>
<td align="center"> containers * RAM-per-container = 6 * 2048 = 12288</td>
</tr>
<tr>
<td align="center">2 </td>
<td align="center">yarn-site.xml    </td>
<td align="center"> yarn.scheduler.minimum-allocation-mb   </td>
<td align="center"> RAM-per-container = 2048</td>
</tr>
<tr>
<td align="center">3 </td>
<td align="center">yarn-site.xml     </td>
<td align="center"> yarn.scheduler.maximum-allocation-mb  </td>
<td align="center"> containers * RAM-per-container = 6 * 2048 = 12288</td>
</tr>
<tr>
<td align="center">4 </td>
<td align="center">mapred-site.xml  </td>
<td align="center"> mapreduce.map.memory.mb    </td>
<td align="center"> RAM-per-container = 2048 * 0.25 = 512</td>
</tr>
<tr>
<td align="center">5 </td>
<td align="center">mapred-site.xml  </td>
<td align="center"> mapreduce.reduce.memory.mb     </td>
<td align="center"> 2 * RAM-per-container = 2 * 2048 * 1.5 = 6144</td>
</tr>
<tr>
<td align="center">6 </td>
<td align="center">mapred-site.xml  </td>
<td align="center"> mapreduce.map.java.opts    </td>
<td align="center"> -Xmx512m * 0.8 = 384</td>
</tr>
<tr>
<td align="center">7 </td>
<td align="center">mapred-site.xml  </td>
<td align="center"> mapreduce.reduce.java.opts     </td>
<td align="center"> -Xmx6144m * 0.8 = 5120</td>
</tr>
<tr>
<td align="center">8 </td>
<td align="center">hadoop-env.sh</td>
<td align="center">HADOOP_DATANODE_OPTS</td>
<td align="center">-Xmx12g</td>
</tr>
</tbody>
</table>


<h4>Property setting to make Druid faster</h4>

<h5>JVM Heap</h5>

<p>Broker nodes can use the JVM heap as a query cache and thus, the size of the heap will affect on the number of results that can be cached.</p>

<p>Historical nodes use off-heap memory to store intermediate results, and by default, all segments are memory mapped before they can be queried. The more off-heap memory is available, the more segments can be served without the possibility of data being paged onto disk.</p>

<p>Coordinator nodes do not require off-heap memory and the heap is used for loading information about all segments to determine what segments need to be loaded, dropped, moved, or replicated.</p>

<table>
<thead>
<tr>
<th align="center">No.</th>
<th align="center">Node </th>
<th align="center"> Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1 </td>
<td align="center"> Broker nodes </td>
<td align="center"> -Xmx8192m</td>
</tr>
<tr>
<td align="center">2 </td>
<td align="center"> Historical nodes </td>
<td align="center"> -Xmx4096m</td>
</tr>
<tr>
<td align="center">3 </td>
<td align="center"> Coordinator nodes </td>
<td align="center"> -Xmx4096m</td>
</tr>
</tbody>
</table>


<h5>Historical Nodes</h5>

<table>
<thead>
<tr>
<th align="center">No.</th>
<th align="center">Property </th>
<th align="center"> Description </th>
<th align="center"> Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1 </td>
<td align="center">druid.processing.buffer.sizeBytes</td>
<td align="center">This specifies a buffer size for the storage of intermediate results. The computation engine in both the Historical and Realtime nodes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.</td>
<td align="center"><strong>2147483648(2GB)</strong></td>
</tr>
<tr>
<td align="center">2 </td>
<td align="center">druid.processing.numThreads</td>
<td align="center">The number of processing threads to have available for parallel processing of segments.</td>
<td align="center"> <strong>5</strong></td>
</tr>
<tr>
<td align="center">3 </td>
<td align="center">druid.segmentCache.locations</td>
<td align="center">Segments assigned to a Historical node are first stored on the local file system (in a disk cache) and then served by the Historical node. These locations define where that local cache resides.</td>
<td align="center"> <strong>40000000000(40GB)</strong></td>
</tr>
</tbody>
</table>


<h2>Performance</h2>

<h3>Query Latency</h3>

<h4>Queries Construction</h4>

<p>Druid has its own query languages and accepts queries as POST requests. The body of the POST request is a JSON object containing key-value pairs specifying various query parameters.</p>

<p>We developed a tool to construct 100 queries with random filters.</p>

<h4>Latency</h4>

<table>
<thead>
<tr>
<th align="center">Metrics </th>
<th align="center"> Cost Time</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Average(100 random queries)</td>
<td align="center"> 1.708s</td>
</tr>
<tr>
<td align="center">Full table scan, no filters </td>
<td align="center"> 3.799s</td>
</tr>
<tr>
<td align="center">Query with hundreds of segment id. </td>
<td align="center"> 4.843s</td>
</tr>
</tbody>
</table>


<h3>Building index</h3>

<p>Druid would run three Hadoop Jobs for finishing index building.</p>

<h4>Cost Time</h4>

<p>It took about <strong>1hour 47mins</strong> to finish index building.</p>

<h4>CPU and I/O</h4>

<p>Building index should be an <strong>IO bound</strong> task through analysing the CPU and I/O status during Map and Reduce. I/O writing and reading during both map and reduce tasks is almost 100%. Map tasks occupied more CPU resource than Reduce tasks and Reduce tasks cost more memory.</p>

<p>The number of Reduce tasks depends on the Granularity setting in Druid index task configuration. For instance, it would be 24 reduce tasks if we set granularity as HOUR.</p>

<h3>Disk and Memory Usage by Druid</h3>

<p>Raw data in HDFS could be removed after finishing index building.</p>

<table>
<thead>
<tr>
<th align="center">No.</th>
<th align="center">Nodes Hostname </th>
<th align="center"> Disk usage</th>
<th align="center"> Memory Usage </th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1 </td>
<td align="center">raptorsim0012.rm.bf1.yahoo.com </td>
<td align="center"> 9.2G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">2 </td>
<td align="center">raptorsim0013.rm.bf1.yahoo.com </td>
<td align="center"> 8.6G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">3 </td>
<td align="center">raptorsim0014.rm.bf1.yahoo.com </td>
<td align="center"> 8.5G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">4 </td>
<td align="center">raptorsim0015.rm.bf1.yahoo.com </td>
<td align="center"> 8.5G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">5 </td>
<td align="center">raptorsim0017.rm.bf1.yahoo.com </td>
<td align="center"> 8.3G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">6 </td>
<td align="center">raptorsim0018.rm.bf1.yahoo.com </td>
<td align="center"> 8.8G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">7 </td>
<td align="center">raptorsim0019.rm.bf1.yahoo.com </td>
<td align="center"> 8.4G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">8 </td>
<td align="center">raptorsim0008.rm.bf1.yahoo.com </td>
<td align="center"> 8.7G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">9 </td>
<td align="center">raptorsim0009.rm.bf1.yahoo.com </td>
<td align="center"> 8.5G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">10 </td>
<td align="center">raptorsim0010.rm.bf1.yahoo.com </td>
<td align="center"> 8.3G </td>
<td align="center"> 10G</td>
</tr>
<tr>
<td align="center">11 </td>
<td align="center">Deep Storage (hdfs)</td>
<td align="center">28.8G(segments data) </td>
<td align="center"> --</td>
</tr>
<tr>
<td align="center">Total</td>
<td align="center">--</td>
<td align="center">114.6G</td>
<td align="center">100G</td>
</tr>
</tbody>
</table>


<h2>Conclusion</h2>

<h3>Strengths of Druid performance</h3>

<ol>
<li>Advantage of Hadoop makes ingest data into Druid faster.</li>
<li>Deep storage (HDFS) makes service highly available. When a historical node fails, others can pull missed data from HDFS to keep serving. Do not lose data.</li>
<li>Support timeseries query. We can get result given any time interval quickly.</li>
<li>The size of processed data after index building is about 3~4 times smaller than raw data due to some compression algorithm. So, we could store more data, and provided services for more days.</li>
<li>Cost less memory. Druid take advantage of memory-mapping technology to make memory usage effective.</li>
</ol>


<h3>Weaknesses of Druid performance</h3>

<ol>
<li>Less documents or instructions could be found.</li>
<li>Druid is memory-based and not so stable. A lot of exceptions occurred due to out of memory error.</li>
<li>There is no monitoring methods to help monitor running status of Druid cluster. We can't guarantee the correctness of querying result.</li>
<li>Consistency and correctness are weak. Historical nodes need pull data from deep storage, so the latency during data transmission and loading to memory causes the problem.</li>
</ol>


<h2>Appendix</h2>

<p><strong>iostat -x 1 10 </strong> during map and reduce.</p>

<h5>Map:</h5>

<pre>
Linux 2.6.32-431.3.1.el6.YAHOO.20140110.x86_64 (raptorsim0017.rm.bf1.yahoo.com)     05/22/14    _x86_64_    (24 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          17.32    0.00    1.38    7.76    0.00   73.54

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              16.00   604.00  263.00   64.00 69440.00  5352.00   228.72    16.80   51.97   2.68  87.50
          
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          23.94    0.00    2.47    0.00    0.00   73.58

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00     0.00   50.00    0.00 11672.00     0.00   233.44     0.06    1.28   1.24   6.20
          
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          52.01    0.00    1.22    0.34    0.00   46.44

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               1.00     0.00  123.00    0.00 29952.00     0.00   243.51     0.27    2.18   1.40  17.20

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          37.32    0.00    3.65    1.05    0.00   57.99

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               6.00    10.00  252.00    6.00 62264.00   120.00   241.80     0.73    2.84   1.83  47.30

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          39.50    0.00    3.40    0.67    0.00   56.44

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               5.00     0.00  149.00    0.00 36480.00     0.00   244.83     0.77    5.19   2.36  35.20

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          62.70    0.00    1.93    1.72    0.00   33.65

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               2.00  2256.00    6.00  193.00  2048.00 19656.00   109.07     8.66   43.51   2.83  56.40

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          46.50    0.00    1.93    0.21    0.00   51.36

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00     0.00   24.00    0.00  4416.00     0.00   184.00     0.12    5.17   3.83   9.20

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          49.77    0.00    2.01    0.42    0.00   47.80

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               2.00     6.00  109.00    3.00 24720.00    72.00   221.36     0.48    4.28   2.95  33.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          46.52    0.00    1.63    3.81    0.00   48.03

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              28.00     0.00  211.00    1.00 56704.00     0.00   267.47     2.82   11.64   4.65  98.60

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          55.95    0.00    1.17    6.78    0.00   36.10

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              35.00     0.00  331.00    0.00 87520.00     0.00   264.41     5.18   15.02   3.02 100.00
</pre>


<h5>Reduce</h5>

<pre>
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           1.73    0.00    0.53    0.85    0.00   96.89

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               1.04   195.55   10.18   10.99  1578.37  2062.87   171.95     0.18    8.46   5.29  11.20

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          10.29    0.00    1.00   13.26    0.00   75.44

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               6.00   394.00   61.00   75.00  5400.00 48096.00   393.35   233.44  905.21   7.35 100.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          11.84    0.00    0.59    9.75    0.00   77.82

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              15.00     0.00  200.00   69.00 13776.00 54464.00   253.68   167.97 1064.57   3.72 100.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          11.44    0.00    1.68   13.74    0.00   73.15

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              17.00  7326.00  225.00   51.00 22816.00 37640.00   219.04   158.86 1649.70   3.63 100.10

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           9.50    0.00    1.21    7.92    0.00   81.37

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              32.00  3680.00  427.00   34.00 48880.00 29336.00   169.67   160.18  671.39   2.17  99.90

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           4.15    0.00    0.75   11.36    0.00   83.73

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda              12.00 27454.00  133.00  166.00 15688.00 130352.00   488.43   225.93  643.60   3.34 100.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.50    0.00    0.38    9.25    0.00   89.87

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00 15968.00   26.00  165.00  3424.00 140464.00   753.34   264.42 1360.66   5.24 100.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.63    0.00    0.21   11.55    0.00   87.61

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00 13316.00   29.00  153.00  4984.00 115752.00   663.38   244.47 1513.18   5.49 100.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.80    0.00    0.42   13.66    0.00   85.13

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00 15027.00   22.00  174.00  3752.00 134056.00   703.10   223.53  956.06   5.11 100.10

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           1.17    0.00    0.29   11.56    0.00   86.97

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sda               1.00 13681.00   30.00  129.00  4760.00 106760.00   701.38   216.07 1236.13   6.29 100.00
</pre>



</body>
</html>