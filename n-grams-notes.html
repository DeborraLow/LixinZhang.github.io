<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="LixinZhang" />
        <meta name="copyright" content="LixinZhang" />

<meta name="keywords" content="NLP, Note, " />
        <title>N—grams Notes - Backyard of LixinZhang
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/css/bootstrap-combined.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="http://lixinzhang.github.io/theme/css/style.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://lixinzhang.github.io/theme/css/solarizedlight.css" media="screen">
        <link rel="shortcut icon" href="http://lixinzhang.github.io/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="apple-touch-icon" href="http://lixinzhang.github.io/theme/images/apple-touch-icon.png" />
        <link rel="apple-touch-icon" sizes="57x57" href="http://lixinzhang.github.io/theme/images/apple-touch-icon-57x57.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="http://lixinzhang.github.io/theme/images/apple-touch-icon-72x72.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="http://lixinzhang.github.io/theme/images/apple-touch-icon-114x114.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="http://lixinzhang.github.io/theme/images/apple-touch-icon-144x144.png" />
        <link rel="icon" href="http://lixinzhang.github.io/theme/images/apple-touch-icon-144x144.png" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="http://lixinzhang.github.io/"><span class=site-name>Backyard of LixinZhang</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="http://lixinzhang.github.io">Home</a></li>
                            <li ><a href="http://lixinzhang.github.io/book">Notes</a></li>
                            <li ><a href="http://lixinzhang.github.io/categories.html">Categories</a></li>
                            <li ><a href="http://lixinzhang.github.io/tags.html">Tags</a></li>
                            <li ><a href="http://lixinzhang.github.io/archives.html">Archives</a></li>
                            <li ><a href="http://lixinzhang.github.io/resume.pdf">Resume</a></li>
                            <li><form class="navbar-search" action="http://lixinzhang.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page_header span10 offset2">
    <h1><a href="http://lixinzhang.github.io/n-grams-notes.html"> N—grams Notes  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            <h2>N-gram</h2>
<h3>Goal</h3>
<p>Assign a probability to a sentence. Compute the probability of a sentence or sequence of words:
$$P(W)=P(w_1,w_2,...w_N)$$</p>
<p>The Chain Rule applied to compute joint probability of words in sentence </p>
<p>$$P(w_1w_2w_3...w_n) = \prod_{i=1}^{n} P(w_i|w_1w_2...w_{i-1})$$</p>
<h3>Markov Assumption</h3>
<p>$$P(w_1w_2...w_n) \thickapprox \prod_{i=1}^{n} P(w_i|w_{i-k}...w_{i_1})$$
In other words, we approximate each component in the product :
$$P(w_i|w_1w_2...w_{i-1}) \thickapprox P(w_i|w_{i-k}...w_{i_1})$$</p>
<h3>Ngram</h3>
<h4>Model</h4>
<ul>
<li><code>Unigram</code>:simplest case
$$P(w_1w_2...w_n) \thickapprox \prod_{i=1}^{n} P(w_i)$$</li>
<li><code>Bigram model</code> : Condition on the previous word:
$$P(w_i|w_{i-k}...w_{i_1}) \thickapprox P(w_i|w_{i-1})$$</li>
<li>Extend to trigrams,4-grams,5-grams</li>
</ul>
<h4>Estimating bigram probabilities</h4>
<ul>
<li>The Maximum Likelihood Estimate(最大释然估计)
$$P(w_i|w_{i-1}) = \frac{count(w_{i-1}, w_i)}{count(w_{i-1})}$$</li>
<li>It would be $$$zero$$$ when $$$count(w_{i-1}, w_i)=0$$$. So, in order to avoid underflow, we do everything in <code>log</code> space, also adding is faster than multiplying.</li>
<li>$$log(p_1p_2p_3p_4) = log(p_1)+log(p_2)+log(p_3)+log(p_4)$$</li>
</ul>
<h3>Evaluation</h3>
<h4>Perplexity</h4>
<blockquote>
<p>In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models.</p>
</blockquote>
<p>The best language model is one that best predicts an unseen test set. Give the highest <code>P(sentence)</code>.
Perplexity is the inverse probability of the test set, normalised by the number of words. Minimizing perplexity is the same as maximising probability.
$$PP(W)=P(w_1w_2...w_N)^{-\frac{1}{N}}$$</p>
<p>Lower perplexity = better model</p>
<h3>The perils(危险) of overfitting</h3>
<ul>
<li>N-grams only work well for word prediction if the test corpus looks like the training corpus.<ul>
<li>In real life, it often doesn't</li>
<li>We need to train robust models that generalise!</li>
<li>One kind of generalization:zeros<ul>
<li>Things that don't ever occur in the training set, but occur in the test set.  </li>
</ul>
</li>
<li>Zero probability bigrams<ul>
<li>mean that we will assign 0 probability to the test set!</li>
<li>can not compute <code>perplexity</code>(can't divide by 0)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Smoothing</h2>
<h3>Add-one(Laplace) smoothing(拉普拉斯平滑)</h3>
<ul>
<li>Pretend we saw each word one more time than we did.（比实际多一次）</li>
<li>
<p>MLE estimate
  $$P_{MLE}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)}{c(w_{i-1})}$$</p>
</li>
<li>
<p>MLE Add-1 estimate
  $$P_{MLE}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$
  where $$$V=len(sentence)$$$</p>
</li>
<li>Reconstituted(重组) counts</li>
</ul>
<p>$$c(w_{n-1}w_n)=\frac{[C(w_{n-1}w_n)+1] \times C(w_{n-1})}{C(w_{n-1}+V)}$$</p>
<ul>
<li>Add-1estimation is not so good, and there are better methods for <code>N-grams</code>,<code>add-1</code> performs well in domains where the number of zeros isn't so huge.</li>
</ul>
<h3>Backoff(回退) and Interpolation（插值）</h3>
<p>Sometimes it helps to use less context, condition on less context for contexts you have not learned much about.</p>
<h4>Backoff</h4>
<ul>
<li>use trigram if you have good evidence, otherwise bigram, otherwise unigram. (当trigram置信度较低时考虑回退到bigram，还可以再回退到unigram)</li>
</ul>
<h4>Interpolation</h4>
<ul>
<li>mix unigram, bigram, trigram, works better than back off</li>
<li>Linear Interpolation
$$\overline{P}(w_n|w_{n-2}w_{n-1}) = \lambda_1 P(w_n|w_{n-2}w_{n-1}) + \lambda_2 P(w_n|w_{n-1}) + \lambda_3 P(w_n)$$</li>
<li>How to set lambdas?</li>
</ul>
<h3>Unknown words</h3>
<p>Instead: create an unknown word token <em>rare</em>, any training word not in L(a fixed lexicon L of size V) changed to <em>rare</em>, now we train its probabilities like a normal word.</p>
<h3>Huge web-scale n-gram</h3>
<h4>Pruning(修剪)</h4>
<ul>
<li>Only store <code>N-grams</code> with count &gt; threshold, remove singletons of higher-order n-grams</li>
<li>Entropy-based pruning</li>
</ul>
<h4>Efficiency</h4>
<ul>
<li>Efficient data structures like tries</li>
<li>Bloom filters: approximate language models</li>
<li>store words as indexes, not strings</li>
<li>Smoothing for web-scale N-grams ("stupid back off" Brants et al.2007)</li>
</ul>
<h3>Good turing Smoothing</h3>
<ul>
<li>Use the count of things we've seen once to help estimate the count of things we've never seen.Notation:$$$N_c = Frequence of frequency of c$$$, the count of things we've seen c times.</li>
<li>Calculations:<ul>
<li>$$$P_{GT}(unseen) = \frac{N_1}{N}$$$</li>
<li>$$$c^{*}=\frac{(c+1)N_{c+1}}{N_c}$$$</li>
<li>$$$P_{GT}(seen&gt;1) = \frac{c^{*}}{N}$$$</li>
</ul>
</li>
<li>Good_Turing complications(问题)
    *$$$N_c$$$中才并不一定连续，为防止出现跳跃的情况，出现$$$N_c=0$$$，在不可信区间，fit一条直线。 </li>
</ul>
            <aside>
            <nav>
            <ul class="articles_timeline">
 
                <li class="previous_article">« <a href="http://lixinzhang.github.io/qiu-xde-mci-fang-gen.html" title="Previous: 求x的m次方根">求x的m次方根</a></li>
 
                <li class="next_article"><a href="http://lixinzhang.github.io/nlp-spelling-notes.html" title="Next: NLP-Spelling Notes">NLP-Spelling Notes</a> »</li>
            </ul>
            </nav>
            </aside>
<section>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2" 
                href="http://lixinzhang.github.io/n-grams-notes.html/#disqus_thread">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'zhanglixinseu';
        var disqus_identifier = 'http://lixinzhang.github.io/n-grams-notes.html';
    var disqus_url = 'http://lixinzhang.github.io/n-grams-notes.html';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>                </div>
            </div>
        </div>
    </div>
</div>
</section>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
 
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2014-04-21T10:20:00">Apr 21, 2014</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#Note-ref">Note</a> 
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article"> 
                <li><a href="/tags.html#NLP-ref">NLP
                    <span>2</span>
</a></li>
            </ul>

        </div>
        </section>
    </div>
    </article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
<!-- Using MathJax, with the delimiters $ -->
<!-- Conflict with pygments for the .mo and .mi -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  "HTML-CSS": {
  styles: {
  ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
  },
  tex2jax: {inlineMath: [['$$$','$$$'], ['\\(','\\)']],processEscapes: true}
  });
  MathJax.Hub.Register.StartupHook("HTML-CSS Jax Ready",function () {
  var VARIANT = MathJax.OutputJax["HTML-CSS"].FONTDATA.VARIANT;
  VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
  VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
  VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
  VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
  });
  MathJax.Hub.Register.StartupHook("SVG Jax Ready",function () {
  var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;
  VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
  VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
  VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
  VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

<script type="text/javascript">
    var disqus_shortname = 'zhanglixinseu';

    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>
    </body>
</html>